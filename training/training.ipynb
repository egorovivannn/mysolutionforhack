{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH2DATASET = '/mnt/SSD/workspace/atomic_hack/Сварные_швы/dataset/'\n",
    "DF_PATH = '/mnt/SSD/workspace/atomic_hack/df.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import utilities\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import timm\n",
    "import math\n",
    "from transformers import (get_linear_schedule_with_warmup, \n",
    "                          get_cosine_schedule_with_warmup, \n",
    "                          get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "                          get_constant_schedule_with_warmup)\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import random\n",
    "import gc\n",
    "import os\n",
    "import transformers\n",
    "from transformers import CLIPProcessor, CLIPVisionModel,  CLIPVisionConfig\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from pytorch_metric_learning import losses\n",
    "import open_clip\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, window_size=None):\n",
    "        self.length = 0\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def reset(self):\n",
    "        self.length = 0\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        if self.window_size and (self.count >= self.window_size):\n",
    "            self.reset()\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name = 'ViT-H-14-378-quickgelu' \n",
    "    model_data = 'dfn5b'\n",
    "    samples_per_class = 50\n",
    "    min_samples = 4\n",
    "    image_size = 224 \n",
    "    seed = 5\n",
    "    workers = 6\n",
    "    train_batch_size = 4\n",
    "    valid_batch_size = 4\n",
    "    emb_size = 512\n",
    "    vit_bb_lr = {'10': 1.25e-6, '20': 2.5e-6, '26': 5e-6, '32': 10e-6} \n",
    "    vit_bb_wd = 1e-3\n",
    "    hd_lr = 3e-4\n",
    "    hd_wd = 1e-5\n",
    "    autocast = True\n",
    "    n_warmup_steps = 1000\n",
    "    n_epochs = 50\n",
    "    device = torch.device('cuda')\n",
    "    s=30.\n",
    "    m=.45\n",
    "    m_min=.05\n",
    "    acc_steps = 4\n",
    "    global_step = 0\n",
    "    n_classes = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilities.set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RN50', 'openai'),\n",
       " ('RN50', 'yfcc15m'),\n",
       " ('RN50', 'cc12m'),\n",
       " ('RN50-quickgelu', 'openai'),\n",
       " ('RN50-quickgelu', 'yfcc15m'),\n",
       " ('RN50-quickgelu', 'cc12m'),\n",
       " ('RN101', 'openai'),\n",
       " ('RN101', 'yfcc15m'),\n",
       " ('RN101-quickgelu', 'openai'),\n",
       " ('RN101-quickgelu', 'yfcc15m'),\n",
       " ('RN50x4', 'openai'),\n",
       " ('RN50x16', 'openai'),\n",
       " ('RN50x64', 'openai'),\n",
       " ('ViT-B-32', 'openai'),\n",
       " ('ViT-B-32', 'laion400m_e31'),\n",
       " ('ViT-B-32', 'laion400m_e32'),\n",
       " ('ViT-B-32', 'laion2b_e16'),\n",
       " ('ViT-B-32', 'laion2b_s34b_b79k'),\n",
       " ('ViT-B-32', 'datacomp_xl_s13b_b90k'),\n",
       " ('ViT-B-32', 'datacomp_m_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_clip_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_laion_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_image_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_text_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_basic_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_s128m_b4k'),\n",
       " ('ViT-B-32', 'datacomp_s_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_clip_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_laion_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_image_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_text_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_basic_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_s13m_b4k'),\n",
       " ('ViT-B-32-256', 'datacomp_s34b_b86k'),\n",
       " ('ViT-B-32-quickgelu', 'openai'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e31'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e32'),\n",
       " ('ViT-B-32-quickgelu', 'metaclip_400m'),\n",
       " ('ViT-B-32-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-B-16', 'openai'),\n",
       " ('ViT-B-16', 'laion400m_e31'),\n",
       " ('ViT-B-16', 'laion400m_e32'),\n",
       " ('ViT-B-16', 'laion2b_s34b_b88k'),\n",
       " ('ViT-B-16', 'datacomp_xl_s13b_b90k'),\n",
       " ('ViT-B-16', 'datacomp_l_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_clip_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_laion_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_image_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_text_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_basic_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_s1b_b8k'),\n",
       " ('ViT-B-16', 'dfn2b'),\n",
       " ('ViT-B-16-quickgelu', 'metaclip_400m'),\n",
       " ('ViT-B-16-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e31'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'openai'),\n",
       " ('ViT-L-14', 'laion400m_e31'),\n",
       " ('ViT-L-14', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'laion2b_s32b_b82k'),\n",
       " ('ViT-L-14', 'datacomp_xl_s13b_b90k'),\n",
       " ('ViT-L-14', 'commonpool_xl_clip_s13b_b90k'),\n",
       " ('ViT-L-14', 'commonpool_xl_laion_s13b_b90k'),\n",
       " ('ViT-L-14', 'commonpool_xl_s13b_b90k'),\n",
       " ('ViT-L-14-quickgelu', 'metaclip_400m'),\n",
       " ('ViT-L-14-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-L-14-quickgelu', 'dfn2b'),\n",
       " ('ViT-L-14-336', 'openai'),\n",
       " ('ViT-H-14', 'laion2b_s32b_b79k'),\n",
       " ('ViT-H-14-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-H-14-quickgelu', 'dfn5b'),\n",
       " ('ViT-H-14-378-quickgelu', 'dfn5b'),\n",
       " ('ViT-g-14', 'laion2b_s12b_b42k'),\n",
       " ('ViT-g-14', 'laion2b_s34b_b88k'),\n",
       " ('ViT-bigG-14', 'laion2b_s39b_b160k'),\n",
       " ('roberta-ViT-B-32', 'laion2b_s12b_b32k'),\n",
       " ('xlm-roberta-base-ViT-B-32', 'laion5b_s13b_b90k'),\n",
       " ('xlm-roberta-large-ViT-H-14', 'frozen_laion5b_s13b_b90k'),\n",
       " ('convnext_base', 'laion400m_s13b_b51k'),\n",
       " ('convnext_base_w', 'laion2b_s13b_b82k'),\n",
       " ('convnext_base_w', 'laion2b_s13b_b82k_augreg'),\n",
       " ('convnext_base_w', 'laion_aesthetic_s13b_b82k'),\n",
       " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k'),\n",
       " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k_augreg'),\n",
       " ('convnext_large_d', 'laion2b_s26b_b102k_augreg'),\n",
       " ('convnext_large_d_320', 'laion2b_s29b_b131k_ft'),\n",
       " ('convnext_large_d_320', 'laion2b_s29b_b131k_ft_soup'),\n",
       " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg'),\n",
       " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg_rewind'),\n",
       " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg_soup'),\n",
       " ('coca_ViT-B-32', 'laion2b_s13b_b90k'),\n",
       " ('coca_ViT-B-32', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
       " ('coca_ViT-L-14', 'laion2b_s13b_b90k'),\n",
       " ('coca_ViT-L-14', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
       " ('EVA01-g-14', 'laion400m_s11b_b41k'),\n",
       " ('EVA01-g-14-plus', 'merged2b_s11b_b114k'),\n",
       " ('EVA02-B-16', 'merged2b_s8b_b131k'),\n",
       " ('EVA02-L-14', 'merged2b_s4b_b131k'),\n",
       " ('EVA02-L-14-336', 'merged2b_s6b_b61k'),\n",
       " ('EVA02-E-14', 'laion2b_s4b_b115k'),\n",
       " ('EVA02-E-14-plus', 'laion2b_s9b_b144k'),\n",
       " ('ViT-B-16-SigLIP', 'webli'),\n",
       " ('ViT-B-16-SigLIP-256', 'webli'),\n",
       " ('ViT-B-16-SigLIP-i18n-256', 'webli'),\n",
       " ('ViT-B-16-SigLIP-384', 'webli'),\n",
       " ('ViT-B-16-SigLIP-512', 'webli'),\n",
       " ('ViT-L-16-SigLIP-256', 'webli'),\n",
       " ('ViT-L-16-SigLIP-384', 'webli'),\n",
       " ('ViT-SO400M-14-SigLIP', 'webli'),\n",
       " ('ViT-SO400M-14-SigLIP-384', 'webli'),\n",
       " ('ViT-L-14-CLIPA', 'datacomp1b'),\n",
       " ('ViT-L-14-CLIPA-336', 'datacomp1b'),\n",
       " ('ViT-H-14-CLIPA', 'datacomp1b'),\n",
       " ('ViT-H-14-CLIPA-336', 'laion2b'),\n",
       " ('ViT-H-14-CLIPA-336', 'datacomp1b'),\n",
       " ('ViT-bigG-14-CLIPA', 'datacomp1b'),\n",
       " ('ViT-bigG-14-CLIPA-336', 'datacomp1b'),\n",
       " ('nllb-clip-base', 'v1'),\n",
       " ('nllb-clip-large', 'v1'),\n",
       " ('nllb-clip-base-siglip', 'v1'),\n",
       " ('nllb-clip-large-siglip', 'v1')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>x_center</th>\n",
       "      <th>y_center</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>label_idx</th>\n",
       "      <th>label</th>\n",
       "      <th>group</th>\n",
       "      <th>frame</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/mnt/SSD/workspace/atomic_hack/Сварные_швы/dat...</td>\n",
       "      <td>0.524740</td>\n",
       "      <td>0.085781</td>\n",
       "      <td>0.202810</td>\n",
       "      <td>0.039953</td>\n",
       "      <td>914</td>\n",
       "      <td>252</td>\n",
       "      <td>1352</td>\n",
       "      <td>406</td>\n",
       "      <td>2</td>\n",
       "      <td>дефекты геометрии</td>\n",
       "      <td>8</td>\n",
       "      <td>63</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/mnt/SSD/workspace/atomic_hack/Сварные_швы/dat...</td>\n",
       "      <td>0.512523</td>\n",
       "      <td>0.678613</td>\n",
       "      <td>0.249236</td>\n",
       "      <td>0.113984</td>\n",
       "      <td>837</td>\n",
       "      <td>2387</td>\n",
       "      <td>1376</td>\n",
       "      <td>2824</td>\n",
       "      <td>2</td>\n",
       "      <td>дефекты геометрии</td>\n",
       "      <td>8</td>\n",
       "      <td>63</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/mnt/SSD/workspace/atomic_hack/Сварные_швы/dat...</td>\n",
       "      <td>0.464569</td>\n",
       "      <td>0.524089</td>\n",
       "      <td>0.159438</td>\n",
       "      <td>0.101058</td>\n",
       "      <td>831</td>\n",
       "      <td>1818</td>\n",
       "      <td>1175</td>\n",
       "      <td>2206</td>\n",
       "      <td>2</td>\n",
       "      <td>дефекты геометрии</td>\n",
       "      <td>11</td>\n",
       "      <td>79</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/mnt/SSD/workspace/atomic_hack/Сварные_швы/dat...</td>\n",
       "      <td>0.587660</td>\n",
       "      <td>0.222092</td>\n",
       "      <td>0.163714</td>\n",
       "      <td>0.077556</td>\n",
       "      <td>1092</td>\n",
       "      <td>703</td>\n",
       "      <td>1446</td>\n",
       "      <td>1001</td>\n",
       "      <td>2</td>\n",
       "      <td>дефекты геометрии</td>\n",
       "      <td>6</td>\n",
       "      <td>94</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/mnt/SSD/workspace/atomic_hack/Сварные_швы/dat...</td>\n",
       "      <td>0.463653</td>\n",
       "      <td>0.157462</td>\n",
       "      <td>0.108735</td>\n",
       "      <td>0.077556</td>\n",
       "      <td>884</td>\n",
       "      <td>455</td>\n",
       "      <td>1118</td>\n",
       "      <td>753</td>\n",
       "      <td>2</td>\n",
       "      <td>дефекты геометрии</td>\n",
       "      <td>6</td>\n",
       "      <td>94</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               fname  x_center  y_center  \\\n",
       "0  /mnt/SSD/workspace/atomic_hack/Сварные_швы/dat...  0.524740  0.085781   \n",
       "1  /mnt/SSD/workspace/atomic_hack/Сварные_швы/dat...  0.512523  0.678613   \n",
       "2  /mnt/SSD/workspace/atomic_hack/Сварные_швы/dat...  0.464569  0.524089   \n",
       "3  /mnt/SSD/workspace/atomic_hack/Сварные_швы/dat...  0.587660  0.222092   \n",
       "4  /mnt/SSD/workspace/atomic_hack/Сварные_швы/dat...  0.463653  0.157462   \n",
       "\n",
       "      width    height    x1    y1    x2    y2  label_idx              label  \\\n",
       "0  0.202810  0.039953   914   252  1352   406          2  дефекты геометрии   \n",
       "1  0.249236  0.113984   837  2387  1376  2824          2  дефекты геометрии   \n",
       "2  0.159438  0.101058   831  1818  1175  2206          2  дефекты геометрии   \n",
       "3  0.163714  0.077556  1092   703  1446  1001          2  дефекты геометрии   \n",
       "4  0.108735  0.077556   884   455  1118   753          2  дефекты геометрии   \n",
       "\n",
       "   group  frame  split  \n",
       "0      8     63  train  \n",
       "1      8     63  train  \n",
       "2     11     79  train  \n",
       "3      6     94  train  \n",
       "4      6     94  train  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DF_PATH)\n",
    "df['fname'] = df['fname'].apply(lambda x: PATH2DATASET+x)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['split'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df[df['group']==5].index, 'split'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path in np.random.choice(df.fname.unique(), size=250, replace=False):\n",
    "    # df.loc[df[df['fname']==path].index, 'split'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class MetalDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 df, \n",
    "                 mode='train', \n",
    "                 split = 0,\n",
    "                 transform=None\n",
    "                 ):\n",
    "        # if mode=='train':\n",
    "        #     self.df = df[df['split']!=split].reset_index(drop=True)\n",
    "        # else:\n",
    "        #     self.df = df[df['split']==split].reset_index(drop=True)\n",
    "        self.df = df[df['split']==mode].reset_index(drop=True)\n",
    "            \n",
    "        self.augs = transform\n",
    "        self.mode = mode\n",
    "        self.cache_images()\n",
    "        self.paths = list(self.df.fname.unique())\n",
    "\n",
    "    def cache_images(self):\n",
    "        self.idx2meta = {}\n",
    "        idx = 0\n",
    "        scale = 1.1\n",
    "        for path in tqdm(self.df.fname.unique()):\n",
    "            img = cv2.imread(path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            bboxes  = df[df['fname']==path][['x1', 'y1', 'x2', 'y2']].values.tolist()\n",
    "            labels = df[df['fname']==path]['label_idx'].values.tolist()\n",
    "\n",
    "            for bbox, label in zip(bboxes, labels):\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                x1 = int(x1/scale)\n",
    "                x2 = int(x2*scale)    \n",
    "                y1 = int(y1/scale)    \n",
    "                y2 = int(y2*scale)  \n",
    "\n",
    "                crop = img[y1:y2, x1:x2]\n",
    "                \n",
    "                self.idx2meta[idx] = {\n",
    "                    'img': crop,\n",
    "                    'label': label,\n",
    "                }\n",
    "                idx+=1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2meta)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.idx2meta[idx]\n",
    "        img = item['img']\n",
    "        label = item['label']\n",
    "                    \n",
    "        try:\n",
    "            if self.augs:\n",
    "                img = self.augs(image=img)['image']\n",
    "                img = torch.from_numpy(img).permute(2, 0, 1)\n",
    "        except Exception as e:\n",
    "            print(e, path)\n",
    "            img = torch.zeros((3, 224, 224))\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Head, self).__init__()\n",
    "\n",
    "        self.emb = nn.Linear(hidden_size, CFG.emb_size, bias=False)\n",
    "        self.arc = utilities.ArcMarginProduct_subcenter(CFG.emb_size, CFG.n_classes)\n",
    "        self.dropout = utilities.Multisample_Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.dropout(x, self.emb)\n",
    "        \n",
    "        output = self.arc(embeddings)\n",
    "\n",
    "        return output, F.normalize(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vit_backbone):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.vit_backbone = vit_backbone\n",
    "        with torch.no_grad():\n",
    "            emb_size = vit_backbone(torch.randn(2, 3, image_size, image_size))[0].shape[1]\n",
    "        self.head = Head(emb_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "\n",
    "        # x = transforms.functional.resize(images, size=[CFG.image_size, CFG.image_size]) \n",
    "        # x = x/255\n",
    "        # x = transforms.functional.normalize(x,  \n",
    "        #                                      mean=model_transforms.transforms[-1].mean, \n",
    "        #                                      std=model_transforms.transforms[-1].std)\n",
    "\n",
    "        x = self.vit_backbone.encode_image(images)\n",
    "        \n",
    "        return self.head(x)\n",
    "\n",
    "    def get_parameters(self):\n",
    "\n",
    "        parameter_settings = [] \n",
    "        parameter_settings.extend(self.get_parameter_section([(n, p) for n, p in self.vit_backbone.named_parameters()], lr=CFG.vit_bb_lr, wd=CFG.vit_bb_wd)) \n",
    "\n",
    "        parameter_settings.extend(self.get_parameter_section([(n, p) for n, p in self.head.named_parameters()], lr=CFG.hd_lr, wd=CFG.hd_wd)) \n",
    "\n",
    "        return parameter_settings\n",
    "\n",
    "    def get_parameter_section(self, parameters, lr=None, wd=None): \n",
    "        parameter_settings = []\n",
    "\n",
    "\n",
    "        lr_is_dict = isinstance(lr, dict)\n",
    "        wd_is_dict = isinstance(wd, dict)\n",
    "\n",
    "        layer_no = None\n",
    "        for no, (n,p) in enumerate(parameters):\n",
    "            \n",
    "            for split in n.split('.'):\n",
    "                if split.isnumeric():\n",
    "                    layer_no = int(split)\n",
    "            \n",
    "            if not layer_no:\n",
    "                layer_no = 0\n",
    "            \n",
    "            if lr_is_dict:\n",
    "                for k,v in lr.items():\n",
    "                    if layer_no < int(k):\n",
    "                        temp_lr = v\n",
    "                        break\n",
    "            else:\n",
    "                temp_lr = lr\n",
    "\n",
    "            if wd_is_dict:\n",
    "                for k,v in wd.items():\n",
    "                    if layer_no < int(k):\n",
    "                        temp_wd = v\n",
    "                        break\n",
    "            else:\n",
    "                temp_wd = wd\n",
    "\n",
    "            weight_decay = 0.0 if 'bias' in n else temp_wd\n",
    "\n",
    "            parameter_setting = {\"params\" : p, \"lr\" : temp_lr, \"weight_decay\" : temp_wd}\n",
    "\n",
    "            parameter_settings.append(parameter_setting)\n",
    "\n",
    "            #print(f'no {no} | params {n} | lr {temp_lr} | weight_decay {weight_decay} | requires_grad {p.requires_grad}')\n",
    "\n",
    "        return parameter_settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augs(mean, std, image_size, mode='train'):\n",
    "    if mode=='train':\n",
    "        return A.Compose([\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.5),\n",
    "                A.ImageCompression(quality_lower=10, quality_upper=100, p=0.7),\n",
    "                # A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=10, border_mode=0, p=0.7),\n",
    "                A.Resize(image_size, image_size),\n",
    "                # A.Cutout(max_h_size=int(image_size * 0.4), max_w_size=int(image_size * 0.4), num_holes=1, p=0.5),\n",
    "                # A.OneOf([\n",
    "                #     A.ChannelShuffle(),\n",
    "                #     A.ChannelDropout(),\n",
    "                #     A.ColorJitter(),\n",
    "                #     A.ToGray(),\n",
    "                # ], p=0.65),\n",
    "                A.Normalize(mean=mean, std=std, p=1), \n",
    "            ])\n",
    "\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(image_size, image_size),\n",
    "            A.Normalize(mean=mean, std=std, p=1)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_backbone, model_transforms, _ = open_clip.create_model_and_transforms(CFG.model_name, pretrained=CFG.model_data)\n",
    "\n",
    "mean, std = model_transforms.transforms[-1].mean, model_transforms.transforms[-1].std\n",
    "image_size = model_transforms.transforms[0].size[0]\n",
    "\n",
    "model = Model(vit_backbone.cpu()).to(CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 907/907 [00:32<00:00, 27.66it/s]\n",
      "100%|█████████████████████████████████████████| 115/115 [00:04<00:00, 26.50it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MetalDataset(df, mode='train', split=0, transform=get_augs(mean, std, image_size, 'train'))\n",
    "valid_dataset = MetalDataset(df, mode='val', split=0, transform=get_augs(mean, std, image_size, 'val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = CFG.train_batch_size, num_workers=CFG.workers, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size = CFG.valid_batch_size, num_workers=CFG.workers, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ArcFace_criterion(logits_m, target, margins):\n",
    "    arc = utilities.ArcFaceLossAdaptiveMargin(margins=margins, s=CFG.s)\n",
    "    loss_m = arc(logits_m, target, CFG.n_classes)\n",
    "    return loss_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, scaler, scheduler, epoch, pbar_draw=False):\n",
    "    model.train()\n",
    "    loss_metrics = AverageMeter()\n",
    "    criterion = ArcFace_criterion\n",
    "\n",
    "    tmp = np.sqrt(1 / np.sqrt(value_counts))\n",
    "    margins = (tmp - tmp.min()) / (tmp.max() - tmp.min()) * CFG.m + CFG.m_min\n",
    "        \n",
    "    bar = tqdm(train_loader, disable=not pbar_draw)\n",
    "#     bar = tqdm(train_loader, disable=False)\n",
    "    for step, data in enumerate(bar):\n",
    "        step += 1\n",
    "        images = data[0].to(CFG.device, dtype=torch.float)\n",
    "        labels = data[1].to(CFG.device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.autocast):\n",
    "            outputs, features = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels, margins)\n",
    "        loss_metrics.update(loss.item(), batch_size)\n",
    "        loss = loss / CFG.acc_steps\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if step % CFG.acc_steps == 0 or step == len(bar):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            CFG.global_step += 1\n",
    "            \n",
    "        lrs = get_lr_groups(optimizer.param_groups)\n",
    "\n",
    "        loss_avg = loss_metrics.avg\n",
    "\n",
    "        bar.set_postfix(loss=loss_avg, epoch=epoch, lrs=lrs, step=CFG.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.notebook import tqdm\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    device = 'cuda'\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for i, (images, label) in enumerate(loader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images.to(device))[0].argmax(1).cpu().detach().tolist()\n",
    "            preds.extend(outputs)\n",
    "            labels.extend(label)\n",
    "            \n",
    "    metric = f1_score(labels, preds, average='macro')\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df['label_idx'].value_counts().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_groups(param_groups):\n",
    "        groups = sorted(set([param_g['lr'] for param_g in param_groups]))\n",
    "        groups = [\"{:2e}\".format(group) for group in groups]\n",
    "        return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c6fbcc6a4124a7aab9d191c29ae40e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/731 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/anaconda3/envs/env3.12/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.get_parameters())\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=CFG.autocast)\n",
    "steps_per_epoch = math.ceil(len(train_loader) / CFG.acc_steps)\n",
    "num_training_steps = math.ceil(CFG.n_epochs * steps_per_epoch)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                            num_training_steps=num_training_steps,\n",
    "                                            num_warmup_steps=CFG.n_warmup_steps)   \n",
    "CFG.global_step = 0    \n",
    "best_score = 0\n",
    "for epoch in range(math.ceil(CFG.n_epochs)):\n",
    "\n",
    "    train(model, train_loader, optimizer, scaler, scheduler, epoch, pbar_draw=True)\n",
    "    score = validate(model, valid_loader)\n",
    "    print(f'Epoch = {epoch}, score:', score)\n",
    "    # if score>0.97:\n",
    "    #     torch.save(model.state_dict(), f'{CFG.model_name}_{CFG.model_data}_{fold_idx}_{score}.pth')\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if score>best_score and score>0.72:\n",
    "        best_score = score\n",
    "        # torch.save(model.state_dict(), f'best_model_{round(best_score, 3)}.pt')\n",
    "        torch.save(model.state_dict(), f'/mnt/SSD/workspace/atomic_hack/{CFG.model_name}_{CFG.model_data}_{score}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = CFG.model_name.replace('/','-')\n",
    "# torch.save(model.state_dict(), f'../models/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "device = 'cuda'\n",
    "preds = []\n",
    "labels = []\n",
    "for i, (images, label) in enumerate(valid_loader):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images.to(device))[0].argmax(1).cpu().detach().tolist()\n",
    "        preds.extend(outputs)\n",
    "        labels.extend(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2label = {\n",
    "    0: 'прилегающие дефекты', \n",
    "    1: 'дефекты целостности',\n",
    "    2: 'дефекты геометрии',\n",
    "    3: 'дефекты постобработки',\n",
    "    4: 'дефекты невыполнения'\n",
    "}\n",
    "\n",
    "labels = list(idx2label.values())\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_cm = pd.DataFrame(cm, labels, labels)\n",
    "# plt.figure(figsize=(10,7))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt='.3g') # font siz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4ee870ab444af8a8689fba9fdb6a16993f9af4d6f8c51486b98fd7ee4129479"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
